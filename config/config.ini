[General]
llm_service = ollama
llm_api_url = http://localhost:11434/api/chat
model_name = mistral
verbosity = 1
; llm_service = llamafile
; llm_api_url = "http://localhost:8080/v1"
; model_name = mistral
